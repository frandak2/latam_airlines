{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creacion de Data aumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "import latam_airlines.utils.paths as path\n",
    "from latam_airlines.utils.latam_utils import check_quality\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leemos el dataset de training\n",
    "data_train = pd.read_csv(path.data_processed_dir('data_train.csv'))\n",
    "data_train.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para saber cuales serian las variables que mas pueden aporatar al modelo podemos hacer un test estadistico, Como la variable objetivo (y) es categórica, en lugar de utilizar un ANOVA para hacer una comparación de medias de las variables independientes, se puede utilizar un test estadístico para comparar la distribución de las variables independientes para cada categoría de la variable objetivo. Una opción popular es el test chi-cuadrado, que se puede calcular utilizando la función scipy.stats.chi2_contingency. Este test permite determinar si existe una asociación significativa entre la variable objetivo y cada una de las variables independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data_train.drop('atraso_15', axis=1).columns\n",
    "for col in cols:\n",
    "    if col in ['DIA','MES','AÑO', 'HORA','MIN']:\n",
    "        data_train[col] = data_train[col].astype(int).astype(str)\n",
    "    else:\n",
    "        data_train[col] = data_train[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "p_value = []\n",
    "cols = data_train.drop('atraso_15', axis=1).columns\n",
    "for col in cols:\n",
    "    # Crea las tablas de contingencia para cada variable independiente\n",
    "    contingency_table_1 = pd.crosstab(data_train['atraso_15'], data_train[col])\n",
    "\n",
    "    # Realiza la prueba de chi-cuadrado para cada tabla de contingencia\n",
    "    chi2, p_value_1, dof, expected_values_1 = chi2_contingency(contingency_table_1)\n",
    "    if p_value_1< 0.05:\n",
    "        p_value.append({col:p_value_1})\n",
    "        # Imprime los resultados\n",
    "        print(f'P-value variable {col}: {p_value_1}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultados observamos que las variables escogidas son eimportante para poder predecir el atraso y nos aportan informacion relevante, a excepcion del anio, que no supero el test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(columns='AÑO', inplace=True)\n",
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    data= data_train,\n",
    "    x='atraso_15',\n",
    "    kind='count',\n",
    ")\n",
    "plt.xlabel('Atraso mayor a 15 min')\n",
    "plt.ylabel('Numero de Vuelos')\n",
    "plt.title('conteo de vuelos atrasados en el 2017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = ['DIA', 'MES', 'HORA', 'MIN','Vlo_change', 'Emp_change','temporada_alta','periodo_dia','DIANOM', 'MESNOM','Des-I', 'TIPOVUELO', 'OPERA','atraso_15']\n",
    "data_train = data_train[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene las características y la variable objetivo\n",
    "X = data_train.drop(\"atraso_15\", axis=1)\n",
    "y = data_train[\"atraso_15\"]\n",
    "\n",
    "# Crear una pipeline que incluya el ColumnTransformer\n",
    "# Crear un objeto ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler( with_mean=False), ['DIA', 'MES', 'HORA', 'MIN','Vlo_change', 'Emp_change','temporada_alta']),\n",
    "        ('cat-nominal', OneHotEncoder(), ['periodo_dia','DIANOM', 'MESNOM','Des-I', 'TIPOVUELO', 'OPERA'])\n",
    "    ],\n",
    "    )\n",
    "\n",
    "#transformar datos\n",
    "X_transform = preprocessor.fit_transform(X)\n",
    "\n",
    "# Aplica SMOTE para generar datos sintéticos\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_resampled, y_resampled = smote.fit_resample(X_transform, y)\n",
    "\n",
    "# Inversión de la transformación de escalamiento\n",
    "scaler = preprocessor.named_transformers_['num']\n",
    "X_scaled = scaler.inverse_transform(X_resampled[:, :7])\n",
    "\n",
    "# Inversión de la transformación One-hot\n",
    "onehot = preprocessor.named_transformers_['cat-nominal']\n",
    "X_onehot = X_resampled[:, 7:]\n",
    "X_original = onehot.inverse_transform(X_onehot)\n",
    "\n",
    "# Concatena los valores originales y convierte el resultado en un DataFrame\n",
    "X_recovered = np.concatenate([X_scaled.toarray().astype(int), X_original], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_balanced = pd.DataFrame(X_recovered, columns=X.columns)\n",
    "data_train_balanced[\"atraso_15\"] = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    data= data_train_balanced,\n",
    "    x='atraso_15',\n",
    "    kind='count',\n",
    ")\n",
    "plt.xlabel('Atraso mayor a 15 min')\n",
    "plt.ylabel('Numero de Vuelos')\n",
    "plt.title('conteo de vuelos atrasados en el 2017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_quality(data_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_balanced.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda el dataset balanceado en un archivo\n",
    "data_train_balanced.to_csv(path.data_processed_dir('data_train_balanced.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latam_airlines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fce30770c70df1231e58a4ba7ac7c20698e43e2829a791ca48f11e1d9cd05121"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
